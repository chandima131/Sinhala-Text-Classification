{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7258d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: stemming_words, dtype: float64)\n",
      "Series([], Name: LABEL, dtype: object)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User</th>\n",
       "      <th>CREATED_AT</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>stemming_words</th>\n",
       "      <th>LIKE COUNT</th>\n",
       "      <th>FOLLOWERS COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, User, CREATED_AT, LABEL, stemming_words, LIKE COUNT, FOLLOWERS COUNT]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "   \n",
    "    #from sklearn import naive_bayes, metrics, svm\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#     %matplotlib inline\n",
    "\n",
    "df = pd.read_csv('FinalListPreprocessedData.csv')\n",
    "df = df.dropna()\n",
    "X=df['stemming_words']\n",
    "y=df['LABEL']\n",
    "\n",
    "print(X)\n",
    "\n",
    "print(y)\n",
    "\n",
    "y.value_counts()\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4620617f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a947df754e72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcount_Class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LABEL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcount_Class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"blue\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"orange\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"yellow\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Green\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Red\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Bar chart'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    947\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(data, kind, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ax\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"left_ax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_plot_logic_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_plot_logic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_args_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py\u001b[0m in \u001b[0;36m_post_plot_logic\u001b[1;34m(self, ax, data)\u001b[0m\n\u001b[0;32m   1459\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_index_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m         \u001b[0ms_edge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0max_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.25\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlim_offset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1462\u001b[0m         \u001b[0me_edge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0max_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.25\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar_width\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlim_offset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_Class=pd.value_counts(df[\"LABEL\"], sort= True)\n",
    "count_Class.plot(kind= 'bar', color= [\"blue\", \"orange\",\"yellow\",\"Green\",\"Red\"])\n",
    "plt.title('Bar chart')\n",
    "plt.show()\n",
    "\n",
    "count_Class.plot(kind = 'pie',  autopct='%1.0f%%')\n",
    "plt.title('Pie chart')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b3b9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a734a249",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4fe0afebbdd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stemming_words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stemming_words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4211\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4213\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-4fe0afebbdd3>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stemming_words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stemming_words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-9cf1ee415cf9>\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m     return [\n\u001b[0;32m    132\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \"\"\"\n\u001b[0;32m    107\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \"\"\"\n\u001b[1;32m-> 1274\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m         \"\"\"\n\u001b[1;32m-> 1328\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m         \"\"\"\n\u001b[1;32m-> 1328\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1318\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1319\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1357\u001b[0m         \"\"\"\n\u001b[0;32m   1358\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chandima samarakoon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"after_tok\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "df['stemming_words'] = df['stemming_words'].apply(lambda x: clean_text(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text1 = nltk.word_tokenize(df['stemming_words'])\n",
    "nltk.pos_tag(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4179fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f752679",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9317712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  0  1  0  0  5  0]\n",
      " [ 0  1  1  0  0  0  3]\n",
      " [ 3  0 48  0  0 12  2]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 1  0  8  0  0 27  1]\n",
      " [ 2  0 10  0  0  4 18]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         Crime       0.82      0.82      0.82        33\n",
      " Entertainment       1.00      0.20      0.33         5\n",
      "         Other       0.69      0.74      0.71        65\n",
      "   Other,Crime       0.00      0.00      0.00         1\n",
      "Other,Politics       0.00      0.00      0.00         1\n",
      "      Politics       0.56      0.73      0.64        37\n",
      "         Sport       0.75      0.53      0.62        34\n",
      "\n",
      "      accuracy                           0.69       176\n",
      "     macro avg       0.55      0.43      0.45       176\n",
      "  weighted avg       0.70      0.69      0.68       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['stemming_words'] \n",
    "y = df['LABEL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer(analyzer = 'word', max_features = 10000, ngram_range=(1,2))),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "#\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = text_clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18917a1",
   "metadata": {},
   "source": [
    "# Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb26bbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  2  0  0  5  0]\n",
      " [ 0  0  2  0  0  0  3]\n",
      " [ 1  0 54  0  0 10  0]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  0 12  0  0 25  0]\n",
      " [ 0  0 16  0  0  5 13]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         Crime       0.96      0.79      0.87        33\n",
      " Entertainment       0.00      0.00      0.00         5\n",
      "         Other       0.61      0.83      0.71        65\n",
      "   Other,Crime       0.00      0.00      0.00         1\n",
      "Other,Politics       0.00      0.00      0.00         1\n",
      "      Politics       0.56      0.68      0.61        37\n",
      "         Sport       0.81      0.38      0.52        34\n",
      "\n",
      "      accuracy                           0.67       176\n",
      "     macro avg       0.42      0.38      0.39       176\n",
      "  weighted avg       0.68      0.67      0.65       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['stemming_words'] \n",
    "y = df['LABEL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train_tfidf,y_train)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "logistic_text_clf = Pipeline([('tfidf', TfidfVectorizer(analyzer = 'word', max_features = 10000, ngram_range=(1,2))),\n",
    "                     ('model', LogisticRegression()),\n",
    "])\n",
    "#\n",
    "# Feed the training data through the pipeline\n",
    "logistic_text_clf.fit(X_train, y_train)\n",
    "\n",
    "logistic_predictions = logistic_text_clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,logistic_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test,logistic_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee59032",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29bf2013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24  0  4  0  0  5  0]\n",
      " [ 0  0  2  0  0  0  3]\n",
      " [ 1  0 57  0  0  7  0]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  0 13  0  0 24  0]\n",
      " [ 0  0 25  0  0  2  7]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         Crime       0.96      0.73      0.83        33\n",
      " Entertainment       0.00      0.00      0.00         5\n",
      "         Other       0.55      0.88      0.68        65\n",
      "   Other,Crime       0.00      0.00      0.00         1\n",
      "Other,Politics       0.00      0.00      0.00         1\n",
      "      Politics       0.63      0.65      0.64        37\n",
      "         Sport       0.70      0.21      0.32        34\n",
      "\n",
      "      accuracy                           0.64       176\n",
      "     macro avg       0.41      0.35      0.35       176\n",
      "  weighted avg       0.65      0.64      0.60       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['stemming_words'] \n",
    "y = df['LABEL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tfidf,y_train)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "NB_text_clf = Pipeline([('tfidf', TfidfVectorizer(analyzer = 'word', max_features = 10000, ngram_range=(1,2))),\n",
    "                     ('mnb', MultinomialNB()),\n",
    "])\n",
    "#\n",
    "# Feed the training data through the pipeline\n",
    "NB_text_clf.fit(X_train, y_train)\n",
    "\n",
    "NB_predictions = NB_text_clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,NB_predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test,NB_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b441b1",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d6d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  1  3  0  0  2  0]\n",
      " [ 0  2  1  0  0  0  2]\n",
      " [ 7  2 45  0  0  7  4]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0]\n",
      " [ 5  3  8  0  0 19  2]\n",
      " [ 7  1 12  0  0  1 13]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         Crime       0.59      0.82      0.68        33\n",
      " Entertainment       0.22      0.40      0.29         5\n",
      "         Other       0.64      0.69      0.67        65\n",
      "   Other,Crime       0.00      0.00      0.00         1\n",
      "Other,Politics       0.00      0.00      0.00         1\n",
      "      Politics       0.63      0.51      0.57        37\n",
      "         Sport       0.62      0.38      0.47        34\n",
      "\n",
      "      accuracy                           0.60       176\n",
      "     macro avg       0.39      0.40      0.38       176\n",
      "  weighted avg       0.61      0.60      0.59       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['stemming_words'] \n",
    "y = df['LABEL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train_tfidf,y_train)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "KNN_text_clf = Pipeline([('tfidf', TfidfVectorizer(analyzer = 'word', max_features = 10000, ngram_range=(1,3))),\n",
    "                     ('neigh', KNeighborsClassifier(n_neighbors=3)),\n",
    "])\n",
    "#\n",
    "# Feed the training data through the pipeline\n",
    "KNN_text_clf.fit(X_train, y_train)\n",
    "KNN_predictions = KNN_text_clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.confusion_matrix(y_test,KNN_predictions))\n",
    "print(metrics.classification_report(y_test,KNN_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18482416",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4b02242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28  0  0  0  0  5  0]\n",
      " [ 0  1  1  0  0  0  3]\n",
      " [ 5  0 45  0  0 14  1]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0]\n",
      " [ 1  0  7  0  0 28  1]\n",
      " [ 2  0  9  0  0  5 18]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         Crime       0.78      0.85      0.81        33\n",
      " Entertainment       1.00      0.20      0.33         5\n",
      "         Other       0.70      0.69      0.70        65\n",
      "   Other,Crime       0.00      0.00      0.00         1\n",
      "Other,Politics       0.00      0.00      0.00         1\n",
      "      Politics       0.54      0.76      0.63        37\n",
      "         Sport       0.78      0.53      0.63        34\n",
      "\n",
      "      accuracy                           0.68       176\n",
      "     macro avg       0.54      0.43      0.44       176\n",
      "  weighted avg       0.70      0.68      0.67       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = df['stemming_words'] \n",
    "y = df['LABEL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape\n",
    "\n",
    "\n",
    "sdg = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "sdg.fit(X_train_tfidf,y_train)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "SDG_text_clf = Pipeline([('tfidf', TfidfVectorizer(analyzer = 'word', max_features = 10000, ngram_range=(1,3))),\n",
    "                     ('sdg', SGDClassifier(max_iter=1000, tol=1e-3)),\n",
    "])\n",
    "#\n",
    "# Feed the training data through the pipeline\n",
    "SDG_text_clf.fit(X_train, y_train)\n",
    "SDG_predictions = SDG_text_clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.confusion_matrix(y_test,SDG_predictions))\n",
    "print(metrics.classification_report(y_test,SDG_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46faf239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sport']\n",
      "['Crime']\n",
      "['Crime']\n"
     ]
    }
   ],
   "source": [
    "predi = KNN_text_clf.predict([\"ඔලිම්පික් ක්‍රීඩාවේ නිරත වන්නන් ටෝකියෝ බලා පිටත් වේ\"])\n",
    "print(predi)\n",
    "predi = KNN_text_clf.predict([\"ගුරු වැටුප් විෂමතා වලට විසඳුම් ලැබේ\"])\n",
    "print(predi)\n",
    "predi = KNN_text_clf.predict([\" ටෝකියෝ අත්අඩංගුවට පත්වේ\"])\n",
    "print(predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b8f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
